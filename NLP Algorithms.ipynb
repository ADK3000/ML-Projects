{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sam=\"\"\"Mango is a fruit. It is rich in fibres. It is generally harvested in summer.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\arpna\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mango is a fruit.',\n",
       " 'It is rich in fibres.',\n",
       " 'It is generally harvested in summer.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.sent_tokenize(sam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mango',\n",
       " 'is',\n",
       " 'a',\n",
       " 'fruit',\n",
       " '.',\n",
       " 'It',\n",
       " 'is',\n",
       " 'rich',\n",
       " 'in',\n",
       " 'fibres',\n",
       " '.',\n",
       " 'It',\n",
       " 'is',\n",
       " 'generally',\n",
       " 'harvested',\n",
       " 'in',\n",
       " 'summer',\n",
       " '.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.word_tokenize(sam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>A series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId                                             Phrase  \\\n",
       "0         1           1  A series of escapades demonstrating the adage ...   \n",
       "1         2           1  A series of escapades demonstrating the adage ...   \n",
       "2         3           1                                           A series   \n",
       "3         4           1                                                  A   \n",
       "4         5           1                                             series   \n",
       "\n",
       "   Sentiment  \n",
       "0          1  \n",
       "1          2  \n",
       "2          2  \n",
       "3          2  \n",
       "4          2  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(r'F:\\Data science\\Projects\\train.tsv',sep='\\t')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(156060, 4)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "li=[]\n",
    "for i in df.Phrase:\n",
    "    li.append(nltk.word_tokenize(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokenize_word']=li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>tokenize_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[A, series, of, escapades, demonstrating, the,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>2</td>\n",
       "      <td>[A, series, of, escapades, demonstrating, the,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>A series</td>\n",
       "      <td>2</td>\n",
       "      <td>[A, series]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "      <td>[A]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>series</td>\n",
       "      <td>2</td>\n",
       "      <td>[series]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId                                             Phrase  \\\n",
       "0         1           1  A series of escapades demonstrating the adage ...   \n",
       "1         2           1  A series of escapades demonstrating the adage ...   \n",
       "2         3           1                                           A series   \n",
       "3         4           1                                                  A   \n",
       "4         5           1                                             series   \n",
       "\n",
       "   Sentiment                                      tokenize_word  \n",
       "0          1  [A, series, of, escapades, demonstrating, the,...  \n",
       "1          2  [A, series, of, escapades, demonstrating, the,...  \n",
       "2          2                                        [A, series]  \n",
       "3          2                                                [A]  \n",
       "4          2                                           [series]  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'doe'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.stem('does')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'happen'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.stem('happened')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "li3=[]\n",
    "for i in df.tokenize_word:\n",
    "    li2=[]\n",
    "    for j in i:\n",
    "        li2.append(ps.stem(j))\n",
    "    li3.append(li2)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['stemed_word']=li3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>tokenize_word</th>\n",
       "      <th>stemed_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[A, series, of, escapades, demonstrating, the,...</td>\n",
       "      <td>[a, seri, of, escapad, demonstr, the, adag, th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>2</td>\n",
       "      <td>[A, series, of, escapades, demonstrating, the,...</td>\n",
       "      <td>[a, seri, of, escapad, demonstr, the, adag, th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>A series</td>\n",
       "      <td>2</td>\n",
       "      <td>[A, series]</td>\n",
       "      <td>[a, seri]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "      <td>[A]</td>\n",
       "      <td>[a]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>series</td>\n",
       "      <td>2</td>\n",
       "      <td>[series]</td>\n",
       "      <td>[seri]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId                                             Phrase  \\\n",
       "0         1           1  A series of escapades demonstrating the adage ...   \n",
       "1         2           1  A series of escapades demonstrating the adage ...   \n",
       "2         3           1                                           A series   \n",
       "3         4           1                                                  A   \n",
       "4         5           1                                             series   \n",
       "\n",
       "   Sentiment                                      tokenize_word  \\\n",
       "0          1  [A, series, of, escapades, demonstrating, the,...   \n",
       "1          2  [A, series, of, escapades, demonstrating, the,...   \n",
       "2          2                                        [A, series]   \n",
       "3          2                                                [A]   \n",
       "4          2                                           [series]   \n",
       "\n",
       "                                         stemed_word  \n",
       "0  [a, seri, of, escapad, demonstr, the, adag, th...  \n",
       "1  [a, seri, of, escapad, demonstr, the, adag, th...  \n",
       "2                                          [a, seri]  \n",
       "3                                                [a]  \n",
       "4                                             [seri]  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "lem=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\arpna\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'do'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lem.lemmatize('does',pos='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'demonstrate'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lem.lemmatize('demonstrating',pos='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "li5=[]\n",
    "for i in df.tokenize_word:\n",
    "    li4=[]\n",
    "    for j in i:\n",
    "        li4.append(lem.lemmatize(j,pos='v'))\n",
    "    li5.append(li4)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lemmatized_word']=li5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>tokenize_word</th>\n",
       "      <th>stemed_word</th>\n",
       "      <th>lemmatized_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[A, series, of, escapades, demonstrating, the,...</td>\n",
       "      <td>[a, seri, of, escapad, demonstr, the, adag, th...</td>\n",
       "      <td>[A, series, of, escapades, demonstrate, the, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>2</td>\n",
       "      <td>[A, series, of, escapades, demonstrating, the,...</td>\n",
       "      <td>[a, seri, of, escapad, demonstr, the, adag, th...</td>\n",
       "      <td>[A, series, of, escapades, demonstrate, the, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>A series</td>\n",
       "      <td>2</td>\n",
       "      <td>[A, series]</td>\n",
       "      <td>[a, seri]</td>\n",
       "      <td>[A, series]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "      <td>[A]</td>\n",
       "      <td>[a]</td>\n",
       "      <td>[A]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>series</td>\n",
       "      <td>2</td>\n",
       "      <td>[series]</td>\n",
       "      <td>[seri]</td>\n",
       "      <td>[series]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId                                             Phrase  \\\n",
       "0         1           1  A series of escapades demonstrating the adage ...   \n",
       "1         2           1  A series of escapades demonstrating the adage ...   \n",
       "2         3           1                                           A series   \n",
       "3         4           1                                                  A   \n",
       "4         5           1                                             series   \n",
       "\n",
       "   Sentiment                                      tokenize_word  \\\n",
       "0          1  [A, series, of, escapades, demonstrating, the,...   \n",
       "1          2  [A, series, of, escapades, demonstrating, the,...   \n",
       "2          2                                        [A, series]   \n",
       "3          2                                                [A]   \n",
       "4          2                                           [series]   \n",
       "\n",
       "                                         stemed_word  \\\n",
       "0  [a, seri, of, escapad, demonstr, the, adag, th...   \n",
       "1  [a, seri, of, escapad, demonstr, the, adag, th...   \n",
       "2                                          [a, seri]   \n",
       "3                                                [a]   \n",
       "4                                             [seri]   \n",
       "\n",
       "                                     lemmatized_word  \n",
       "0  [A, series, of, escapades, demonstrate, the, a...  \n",
       "1  [A, series, of, escapades, demonstrate, the, a...  \n",
       "2                                        [A, series]  \n",
       "3                                                [A]  \n",
       "4                                           [series]  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\arpna\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A', 'DT'),\n",
       " ('series', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('escapades', 'NNS'),\n",
       " ('demonstrate', 'VBP'),\n",
       " ('the', 'DT'),\n",
       " ('adage', 'NN'),\n",
       " ('that', 'WDT'),\n",
       " ('what', 'WP'),\n",
       " ('be', 'VB'),\n",
       " ('good', 'JJ'),\n",
       " ('for', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('goose', 'NN'),\n",
       " ('be', 'VB'),\n",
       " ('also', 'RB'),\n",
       " ('good', 'JJ'),\n",
       " ('for', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('gander', 'NN'),\n",
       " (',', ','),\n",
       " ('some', 'DT'),\n",
       " ('of', 'IN'),\n",
       " ('which', 'WDT'),\n",
       " ('occasionally', 'RB'),\n",
       " ('amuse', 'VBP'),\n",
       " ('but', 'CC'),\n",
       " ('none', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('which', 'WDT'),\n",
       " ('amount', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('much', 'RB'),\n",
       " ('of', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('story', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(df['lemmatized_word'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\arpna\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop=stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "li6=['i','am','the','only','one','son']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one\n",
      "son\n"
     ]
    }
   ],
   "source": [
    "for i in li6:\n",
    "    if i not in stop:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "li7=[]\n",
    "for i in df.lemmatized_word:\n",
    "    li8=[]\n",
    "    for j in i:\n",
    "        if j.lower() not in stop:\n",
    "          li8.append(j)  \n",
    "    li7.append(li8)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['withoutstopwords']=li7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>tokenize_word</th>\n",
       "      <th>stemed_word</th>\n",
       "      <th>lemmatized_word</th>\n",
       "      <th>withoutstopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[A, series, of, escapades, demonstrating, the,...</td>\n",
       "      <td>[a, seri, of, escapad, demonstr, the, adag, th...</td>\n",
       "      <td>[A, series, of, escapades, demonstrate, the, a...</td>\n",
       "      <td>[series, escapades, demonstrate, adage, good, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>2</td>\n",
       "      <td>[A, series, of, escapades, demonstrating, the,...</td>\n",
       "      <td>[a, seri, of, escapad, demonstr, the, adag, th...</td>\n",
       "      <td>[A, series, of, escapades, demonstrate, the, a...</td>\n",
       "      <td>[series, escapades, demonstrate, adage, good, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>A series</td>\n",
       "      <td>2</td>\n",
       "      <td>[A, series]</td>\n",
       "      <td>[a, seri]</td>\n",
       "      <td>[A, series]</td>\n",
       "      <td>[series]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "      <td>[A]</td>\n",
       "      <td>[a]</td>\n",
       "      <td>[A]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>series</td>\n",
       "      <td>2</td>\n",
       "      <td>[series]</td>\n",
       "      <td>[seri]</td>\n",
       "      <td>[series]</td>\n",
       "      <td>[series]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId                                             Phrase  \\\n",
       "0         1           1  A series of escapades demonstrating the adage ...   \n",
       "1         2           1  A series of escapades demonstrating the adage ...   \n",
       "2         3           1                                           A series   \n",
       "3         4           1                                                  A   \n",
       "4         5           1                                             series   \n",
       "\n",
       "   Sentiment                                      tokenize_word  \\\n",
       "0          1  [A, series, of, escapades, demonstrating, the,...   \n",
       "1          2  [A, series, of, escapades, demonstrating, the,...   \n",
       "2          2                                        [A, series]   \n",
       "3          2                                                [A]   \n",
       "4          2                                           [series]   \n",
       "\n",
       "                                         stemed_word  \\\n",
       "0  [a, seri, of, escapad, demonstr, the, adag, th...   \n",
       "1  [a, seri, of, escapad, demonstr, the, adag, th...   \n",
       "2                                          [a, seri]   \n",
       "3                                                [a]   \n",
       "4                                             [seri]   \n",
       "\n",
       "                                     lemmatized_word  \\\n",
       "0  [A, series, of, escapades, demonstrate, the, a...   \n",
       "1  [A, series, of, escapades, demonstrate, the, a...   \n",
       "2                                        [A, series]   \n",
       "3                                                [A]   \n",
       "4                                           [series]   \n",
       "\n",
       "                                    withoutstopwords  \n",
       "0  [series, escapades, demonstrate, adage, good, ...  \n",
       "1  [series, escapades, demonstrate, adage, good, ...  \n",
       "2                                           [series]  \n",
       "3                                                 []  \n",
       "4                                           [series]  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv=CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "v=cv.fit_transform(df['Phrase'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(156060, 15240)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scipy.sparse.csr_matrix.toarray(v).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>tokenize_word</th>\n",
       "      <th>stemed_word</th>\n",
       "      <th>lemmatized_word</th>\n",
       "      <th>withoutstopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[A, series, of, escapades, demonstrating, the,...</td>\n",
       "      <td>[a, seri, of, escapad, demonstr, the, adag, th...</td>\n",
       "      <td>[A, series, of, escapades, demonstrate, the, a...</td>\n",
       "      <td>[series, escapades, demonstrate, adage, good, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>2</td>\n",
       "      <td>[A, series, of, escapades, demonstrating, the,...</td>\n",
       "      <td>[a, seri, of, escapad, demonstr, the, adag, th...</td>\n",
       "      <td>[A, series, of, escapades, demonstrate, the, a...</td>\n",
       "      <td>[series, escapades, demonstrate, adage, good, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>A series</td>\n",
       "      <td>2</td>\n",
       "      <td>[A, series]</td>\n",
       "      <td>[a, seri]</td>\n",
       "      <td>[A, series]</td>\n",
       "      <td>[series]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "      <td>[A]</td>\n",
       "      <td>[a]</td>\n",
       "      <td>[A]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>series</td>\n",
       "      <td>2</td>\n",
       "      <td>[series]</td>\n",
       "      <td>[seri]</td>\n",
       "      <td>[series]</td>\n",
       "      <td>[series]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId                                             Phrase  \\\n",
       "0         1           1  A series of escapades demonstrating the adage ...   \n",
       "1         2           1  A series of escapades demonstrating the adage ...   \n",
       "2         3           1                                           A series   \n",
       "3         4           1                                                  A   \n",
       "4         5           1                                             series   \n",
       "\n",
       "   Sentiment                                      tokenize_word  \\\n",
       "0          1  [A, series, of, escapades, demonstrating, the,...   \n",
       "1          2  [A, series, of, escapades, demonstrating, the,...   \n",
       "2          2                                        [A, series]   \n",
       "3          2                                                [A]   \n",
       "4          2                                           [series]   \n",
       "\n",
       "                                         stemed_word  \\\n",
       "0  [a, seri, of, escapad, demonstr, the, adag, th...   \n",
       "1  [a, seri, of, escapad, demonstr, the, adag, th...   \n",
       "2                                          [a, seri]   \n",
       "3                                                [a]   \n",
       "4                                             [seri]   \n",
       "\n",
       "                                     lemmatized_word  \\\n",
       "0  [A, series, of, escapades, demonstrate, the, a...   \n",
       "1  [A, series, of, escapades, demonstrate, the, a...   \n",
       "2                                        [A, series]   \n",
       "3                                                [A]   \n",
       "4                                           [series]   \n",
       "\n",
       "                                    withoutstopwords  \n",
       "0  [series, escapades, demonstrate, adage, good, ...  \n",
       "1  [series, escapades, demonstrate, adage, good, ...  \n",
       "2                                           [series]  \n",
       "3                                                 []  \n",
       "4                                           [series]  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>tokenize_word</th>\n",
       "      <th>stemed_word</th>\n",
       "      <th>lemmatized_word</th>\n",
       "      <th>withoutstopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>102</td>\n",
       "      <td>3</td>\n",
       "      <td>would have a hard time sitting through this one</td>\n",
       "      <td>0</td>\n",
       "      <td>[would, have, a, hard, time, sitting, through,...</td>\n",
       "      <td>[would, have, a, hard, time, sit, through, thi...</td>\n",
       "      <td>[would, have, a, hard, time, sit, through, thi...</td>\n",
       "      <td>[would, hard, time, sit, one]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>104</td>\n",
       "      <td>3</td>\n",
       "      <td>have a hard time sitting through this one</td>\n",
       "      <td>0</td>\n",
       "      <td>[have, a, hard, time, sitting, through, this, ...</td>\n",
       "      <td>[have, a, hard, time, sit, through, thi, one]</td>\n",
       "      <td>[have, a, hard, time, sit, through, this, one]</td>\n",
       "      <td>[hard, time, sit, one]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>158</td>\n",
       "      <td>5</td>\n",
       "      <td>Aggressive self-glorification and a manipulati...</td>\n",
       "      <td>0</td>\n",
       "      <td>[Aggressive, self-glorification, and, a, manip...</td>\n",
       "      <td>[aggress, self-glorif, and, a, manipul, whitew...</td>\n",
       "      <td>[Aggressive, self-glorification, and, a, manip...</td>\n",
       "      <td>[Aggressive, self-glorification, manipulative,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>160</td>\n",
       "      <td>5</td>\n",
       "      <td>self-glorification and a manipulative whitewash</td>\n",
       "      <td>0</td>\n",
       "      <td>[self-glorification, and, a, manipulative, whi...</td>\n",
       "      <td>[self-glorif, and, a, manipul, whitewash]</td>\n",
       "      <td>[self-glorification, and, a, manipulative, whi...</td>\n",
       "      <td>[self-glorification, manipulative, whitewash]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>202</td>\n",
       "      <td>7</td>\n",
       "      <td>Trouble Every Day is a plodding mess .</td>\n",
       "      <td>0</td>\n",
       "      <td>[Trouble, Every, Day, is, a, plodding, mess, .]</td>\n",
       "      <td>[troubl, everi, day, is, a, plod, mess, .]</td>\n",
       "      <td>[Trouble, Every, Day, be, a, plod, mess, .]</td>\n",
       "      <td>[Trouble, Every, Day, plod, mess, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155965</th>\n",
       "      <td>155966</td>\n",
       "      <td>8539</td>\n",
       "      <td>has turned out nearly 21\\/2 hours of unfocused...</td>\n",
       "      <td>0</td>\n",
       "      <td>[has, turned, out, nearly, 21\\/2, hours, of, u...</td>\n",
       "      <td>[ha, turn, out, nearli, 21\\/2, hour, of, unfoc...</td>\n",
       "      <td>[have, turn, out, nearly, 21\\/2, hours, of, un...</td>\n",
       "      <td>[turn, nearly, 21\\/2, hours, unfocused, ,, exc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155967</th>\n",
       "      <td>155968</td>\n",
       "      <td>8539</td>\n",
       "      <td>turned out nearly 21\\/2 hours of unfocused , e...</td>\n",
       "      <td>0</td>\n",
       "      <td>[turned, out, nearly, 21\\/2, hours, of, unfocu...</td>\n",
       "      <td>[turn, out, nearli, 21\\/2, hour, of, unfocus, ...</td>\n",
       "      <td>[turn, out, nearly, 21\\/2, hours, of, unfocuse...</td>\n",
       "      <td>[turn, nearly, 21\\/2, hours, unfocused, ,, exc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155970</th>\n",
       "      <td>155971</td>\n",
       "      <td>8539</td>\n",
       "      <td>of unfocused , excruciatingly tedious cinema</td>\n",
       "      <td>0</td>\n",
       "      <td>[of, unfocused, ,, excruciatingly, tedious, ci...</td>\n",
       "      <td>[of, unfocus, ,, excruciatingli, tediou, cinema]</td>\n",
       "      <td>[of, unfocused, ,, excruciatingly, tedious, ci...</td>\n",
       "      <td>[unfocused, ,, excruciatingly, tedious, cinema]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155971</th>\n",
       "      <td>155972</td>\n",
       "      <td>8539</td>\n",
       "      <td>unfocused , excruciatingly tedious cinema</td>\n",
       "      <td>0</td>\n",
       "      <td>[unfocused, ,, excruciatingly, tedious, cinema]</td>\n",
       "      <td>[unfocus, ,, excruciatingli, tediou, cinema]</td>\n",
       "      <td>[unfocused, ,, excruciatingly, tedious, cinema]</td>\n",
       "      <td>[unfocused, ,, excruciatingly, tedious, cinema]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155973</th>\n",
       "      <td>155974</td>\n",
       "      <td>8539</td>\n",
       "      <td>, excruciatingly tedious</td>\n",
       "      <td>0</td>\n",
       "      <td>[,, excruciatingly, tedious]</td>\n",
       "      <td>[,, excruciatingli, tediou]</td>\n",
       "      <td>[,, excruciatingly, tedious]</td>\n",
       "      <td>[,, excruciatingly, tedious]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7072 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        PhraseId  SentenceId  \\\n",
       "101          102           3   \n",
       "103          104           3   \n",
       "157          158           5   \n",
       "159          160           5   \n",
       "201          202           7   \n",
       "...          ...         ...   \n",
       "155965    155966        8539   \n",
       "155967    155968        8539   \n",
       "155970    155971        8539   \n",
       "155971    155972        8539   \n",
       "155973    155974        8539   \n",
       "\n",
       "                                                   Phrase  Sentiment  \\\n",
       "101       would have a hard time sitting through this one          0   \n",
       "103             have a hard time sitting through this one          0   \n",
       "157     Aggressive self-glorification and a manipulati...          0   \n",
       "159       self-glorification and a manipulative whitewash          0   \n",
       "201                Trouble Every Day is a plodding mess .          0   \n",
       "...                                                   ...        ...   \n",
       "155965  has turned out nearly 21\\/2 hours of unfocused...          0   \n",
       "155967  turned out nearly 21\\/2 hours of unfocused , e...          0   \n",
       "155970       of unfocused , excruciatingly tedious cinema          0   \n",
       "155971          unfocused , excruciatingly tedious cinema          0   \n",
       "155973                           , excruciatingly tedious          0   \n",
       "\n",
       "                                            tokenize_word  \\\n",
       "101     [would, have, a, hard, time, sitting, through,...   \n",
       "103     [have, a, hard, time, sitting, through, this, ...   \n",
       "157     [Aggressive, self-glorification, and, a, manip...   \n",
       "159     [self-glorification, and, a, manipulative, whi...   \n",
       "201       [Trouble, Every, Day, is, a, plodding, mess, .]   \n",
       "...                                                   ...   \n",
       "155965  [has, turned, out, nearly, 21\\/2, hours, of, u...   \n",
       "155967  [turned, out, nearly, 21\\/2, hours, of, unfocu...   \n",
       "155970  [of, unfocused, ,, excruciatingly, tedious, ci...   \n",
       "155971    [unfocused, ,, excruciatingly, tedious, cinema]   \n",
       "155973                       [,, excruciatingly, tedious]   \n",
       "\n",
       "                                              stemed_word  \\\n",
       "101     [would, have, a, hard, time, sit, through, thi...   \n",
       "103         [have, a, hard, time, sit, through, thi, one]   \n",
       "157     [aggress, self-glorif, and, a, manipul, whitew...   \n",
       "159             [self-glorif, and, a, manipul, whitewash]   \n",
       "201            [troubl, everi, day, is, a, plod, mess, .]   \n",
       "...                                                   ...   \n",
       "155965  [ha, turn, out, nearli, 21\\/2, hour, of, unfoc...   \n",
       "155967  [turn, out, nearli, 21\\/2, hour, of, unfocus, ...   \n",
       "155970   [of, unfocus, ,, excruciatingli, tediou, cinema]   \n",
       "155971       [unfocus, ,, excruciatingli, tediou, cinema]   \n",
       "155973                        [,, excruciatingli, tediou]   \n",
       "\n",
       "                                          lemmatized_word  \\\n",
       "101     [would, have, a, hard, time, sit, through, thi...   \n",
       "103        [have, a, hard, time, sit, through, this, one]   \n",
       "157     [Aggressive, self-glorification, and, a, manip...   \n",
       "159     [self-glorification, and, a, manipulative, whi...   \n",
       "201           [Trouble, Every, Day, be, a, plod, mess, .]   \n",
       "...                                                   ...   \n",
       "155965  [have, turn, out, nearly, 21\\/2, hours, of, un...   \n",
       "155967  [turn, out, nearly, 21\\/2, hours, of, unfocuse...   \n",
       "155970  [of, unfocused, ,, excruciatingly, tedious, ci...   \n",
       "155971    [unfocused, ,, excruciatingly, tedious, cinema]   \n",
       "155973                       [,, excruciatingly, tedious]   \n",
       "\n",
       "                                         withoutstopwords  \n",
       "101                         [would, hard, time, sit, one]  \n",
       "103                                [hard, time, sit, one]  \n",
       "157     [Aggressive, self-glorification, manipulative,...  \n",
       "159         [self-glorification, manipulative, whitewash]  \n",
       "201                  [Trouble, Every, Day, plod, mess, .]  \n",
       "...                                                   ...  \n",
       "155965  [turn, nearly, 21\\/2, hours, unfocused, ,, exc...  \n",
       "155967  [turn, nearly, 21\\/2, hours, unfocused, ,, exc...  \n",
       "155970    [unfocused, ,, excruciatingly, tedious, cinema]  \n",
       "155971    [unfocused, ,, excruciatingly, tedious, cinema]  \n",
       "155973                       [,, excruciatingly, tedious]  \n",
       "\n",
       "[7072 rows x 8 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('Sentiment').get_group(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=v\n",
    "y=df['Sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train, y_test=train_test_split(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "br=BernoulliNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BernoulliNB()"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "br.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5974625144175317"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "br.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "text='shyam is good man'\n",
    "var=cv.transform([text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2], dtype=int64)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "br.predict(var)    #neutral"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lexican Based Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weight and sentiment package (How many positive and negative words are there)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: vaderSentiment in c:\\users\\arpna\\anaconda3\\lib\\site-packages (3.3.2)\n",
      "Requirement already satisfied: requests in c:\\users\\arpna\\anaconda3\\lib\\site-packages (from vaderSentiment) (2.25.1)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\arpna\\anaconda3\\lib\\site-packages (from requests->vaderSentiment) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\arpna\\anaconda3\\lib\\site-packages (from requests->vaderSentiment) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\arpna\\anaconda3\\lib\\site-packages (from requests->vaderSentiment) (2021.5.30)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\arpna\\anaconda3\\lib\\site-packages (from requests->vaderSentiment) (1.26.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install vaderSentiment    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "sia=SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.467, 'neu': 0.533, 'pos': 0.0, 'compound': -0.5423}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sia.polarity_scores('It was a bad day')      #shows how much negative the sentence is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 0.58, 'pos': 0.42, 'compound': 0.4404}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sia.polarity_scores('I am a good girl')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.474, 'neu': 0.526, 'pos': 0.0, 'compound': -0.6705}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sia.polarity_scores('They killed him with a hammer')    #if compound>0.05->it is positive,if compound<-0.05->it is negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 1.0, 'neu': 0.0, 'pos': 0.0, 'compound': -0.6908}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sia.polarity_scores('kill') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Housing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Avg. Area Income</th>\n",
       "      <th>Avg. Area House Age</th>\n",
       "      <th>Avg. Area Number of Rooms</th>\n",
       "      <th>Avg. Area Number of Bedrooms</th>\n",
       "      <th>Area Population</th>\n",
       "      <th>Price</th>\n",
       "      <th>Address</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>79545.458574</td>\n",
       "      <td>5.682861</td>\n",
       "      <td>7.009188</td>\n",
       "      <td>4.09</td>\n",
       "      <td>23086.800503</td>\n",
       "      <td>1.059034e+06</td>\n",
       "      <td>208 Michael Ferry Apt. 674\\nLaurabury, NE 3701...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>79248.642455</td>\n",
       "      <td>6.002900</td>\n",
       "      <td>6.730821</td>\n",
       "      <td>3.09</td>\n",
       "      <td>40173.072174</td>\n",
       "      <td>1.505891e+06</td>\n",
       "      <td>188 Johnson Views Suite 079\\nLake Kathleen, CA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>61287.067179</td>\n",
       "      <td>5.865890</td>\n",
       "      <td>8.512727</td>\n",
       "      <td>5.13</td>\n",
       "      <td>36882.159400</td>\n",
       "      <td>1.058988e+06</td>\n",
       "      <td>9127 Elizabeth Stravenue\\nDanieltown, WI 06482...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>63345.240046</td>\n",
       "      <td>7.188236</td>\n",
       "      <td>5.586729</td>\n",
       "      <td>3.26</td>\n",
       "      <td>34310.242831</td>\n",
       "      <td>1.260617e+06</td>\n",
       "      <td>USS Barnett\\nFPO AP 44820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>59982.197226</td>\n",
       "      <td>5.040555</td>\n",
       "      <td>7.839388</td>\n",
       "      <td>4.23</td>\n",
       "      <td>26354.109472</td>\n",
       "      <td>6.309435e+05</td>\n",
       "      <td>USNS Raymond\\nFPO AE 09386</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Avg. Area Income  Avg. Area House Age  Avg. Area Number of Rooms  \\\n",
       "0      79545.458574             5.682861                   7.009188   \n",
       "1      79248.642455             6.002900                   6.730821   \n",
       "2      61287.067179             5.865890                   8.512727   \n",
       "3      63345.240046             7.188236                   5.586729   \n",
       "4      59982.197226             5.040555                   7.839388   \n",
       "\n",
       "   Avg. Area Number of Bedrooms  Area Population         Price  \\\n",
       "0                          4.09     23086.800503  1.059034e+06   \n",
       "1                          3.09     40173.072174  1.505891e+06   \n",
       "2                          5.13     36882.159400  1.058988e+06   \n",
       "3                          3.26     34310.242831  1.260617e+06   \n",
       "4                          4.23     26354.109472  6.309435e+05   \n",
       "\n",
       "                                             Address  \n",
       "0  208 Michael Ferry Apt. 674\\nLaurabury, NE 3701...  \n",
       "1  188 Johnson Views Suite 079\\nLake Kathleen, CA...  \n",
       "2  9127 Elizabeth Stravenue\\nDanieltown, WI 06482...  \n",
       "3                          USS Barnett\\nFPO AP 44820  \n",
       "4                         USNS Raymond\\nFPO AE 09386  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "house=pd.read_csv(r'F:\\Data science\\Projects\\USA_Housing.csv')\n",
    "house.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 15237)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add= cv.fit_transform(house['Address'])\n",
    "add.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1=add\n",
    "y1=house['Price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lr=LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.fit(X1,y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999999999991932"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(X1,y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_train,X1_test,y1_train, y1_test=train_test_split(X1,y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.fit(X1_train,y1_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_predict=lr.predict(X1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.2788751135397234"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(X1_test, y1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(X1_test, lr_predict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
